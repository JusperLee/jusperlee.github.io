
# ðŸ“ Publications

> ( <sup>*</sup> equal contribution, <sup>#</sup> corresponding author)

## ðŸ§  Neuroscience
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Neural Computation 2022</div><img src='images/nerualcom.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Inferring mechanisms of auditory attentional modulation with deep neural networks](./files/neural.pdf)

Ting-Yu Kuo, Yuanda Liao, **Kai Li**, Bo Hong, Xiaolin Hu.

<a href="https://github.com/liaoyd16/cocktail_lk"><img src="https://img.shields.io/github/stars/liaoyd16/cocktail_lk?style=social&amp;label=Code+Stars" alt=""></a> <strong><span class='show_paper_citations' data='fHkHcMsAAAAJ:ufrVoPGSRksC'></span></strong>

</div>
</div>

## ðŸŽ™ Speech Separation

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Arxiv 2022</div><img src='images/ctcnet.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[An Audio-Visual Speech Separation Model Inspired by Cortico-Thalamo-Cortical Circuits](https://arxiv.org/abs/2212.10744)

**Kai Li**, Fenghua Xie, Hang Chen, Kexin Yuan, Xiaolin Hu.

<a href="https://github.com/JusperLee/CTCNet"><img src="https://img.shields.io/github/stars/JusperLee/CTCNet?style=social&amp;label=Code+Stars" alt=""></a> <strong><span class='show_paper_citations' data='fHkHcMsAAAAJ:roLk4NBRz8UC'></span></strong>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR 2023</div><img src='images/tdanet.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[An efficient encoder-decoder architecture with top-down attention for speech separation](https://arxiv.org/pdf/2209.15200)

**Kai Li**, Runxuan Yang, Xiaolin Hu.

[![çŸ¥ä¹Ž](https://img.shields.io/badge/çŸ¥ä¹Ž-TDANet(ICLR 2023)-0084FF.svg)](https://zhuanlan.zhihu.com/p/605100121) \| 
[**Audio Demo Page**](./project/TDANet) \| <a href="https://github.com/JusperLee/TDANet"><img src="https://img.shields.io/github/stars/JusperLee/TDANet?style=social&amp;label=Code+Stars" alt=""></a> <strong><span class='show_paper_citations' data='fHkHcMsAAAAJ:LkGwnXOMwfcC'></span></strong>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">InterSpeech 2022</div><img src='images/overss.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
  
[On the Use of Deep Mask Estimation Module for Neural Source Separation Systems](http://arxiv.org/pdf/2206.07347v1)

**Kai Li**, Xiaolin Hu, Yi Luo.

<strong><span class='show_paper_citations' data='fHkHcMsAAAAJ:_FxGoFyzp5QC'></span></strong>
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICASSP 2023</div><img src='images/causal.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[On the Design and Training Strategies for RNN-based Online Neural Speech Separation Systems](http://arxiv.org/pdf/2206.07347v1)

**Kai Li**, Yi Luo.

<strong><span class='show_paper_citations' data='fHkHcMsAAAAJ:YsMSGLbcyi4C'></span></strong>
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2021</div><img src='images/afrcnn.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Speech Separation Using an Asynchronous Fully Recurrent Convolutional Neural Network](https://papers.nips.cc/paper/2021/file/be1bc7997695495f756312886f566110-Paper.pdf)

Xiaolin Hu<sup>*, #</sup>, **Kai Li$^*$**, Weiyi Zhang, Yi Luo, Jean-Marie Lemercier, Timo Gerkmann.

[![çŸ¥ä¹Ž](https://img.shields.io/badge/çŸ¥ä¹Ž-AFRCNN(NeuralPS 2021)-0084FF.svg)](https://zhuanlan.zhihu.com/p/508868699) \| [**Audio Demo Page**](./project/AFRCNN) \| [**Speech Enhancement Demo**](./project/AFRCNN-Enh) \| <a href="https://github.com/JusperLee/AFRCNN-For-Speech-Separation"><img src="https://img.shields.io/github/stars/JusperLee/AFRCNN-For-Speech-Separation?style=social&amp;label=Code+Stars" alt=""></a> <strong><span class='show_paper_citations' data='fHkHcMsAAAAJ:zYLM7Y9cAGgC'></span></strong>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Bachelor Degree Thesis</div><img src='images/Bachelor-Degree-Thesis.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Research on Speech Separation Based on Audio Visual Model]()

**Kai Li**, Xiaolin Hu<sup>#</sup>, Jianqiang Huang<sup>#</sup>.

[**Audio Project Page**](./project/Pure-Audio/index.html) \| [**Audio-visual Project Page**](./project/AV-Demo/AV-Model-Demo.html)

</div>
</div>

## ðŸ“· Image Super Resolution
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Submitted IEEE Transactions on Industrial Informatics</div><img src='images/submitted.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Single Image Super-Resolution through Image Pixel Information Clustering and Generative Adversarial Network]()

**Kai Li**, Jianqiang Huang<sup>#</sup>, Jinfang Jia, Yu Zhu, Li Wu, Xiaoying Wang.

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IET Image Processing 2020</div><img src='images/resolution_servey.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[A Survey of Single Image Super Resolution Reconstruction](./files/A_Survey_of_Single_Image_Super_Resolution_Reconstr.pdf)

**Kai Li**, Shenghao Yang, Runting Dong, Jianqiang Huang<sup>#</sup>, Xiaoying Wang.

<strong><span class='show_paper_citations' data='fHkHcMsAAAAJ:u5HHmVD_uO8C'></span></strong>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ISPA 2019</div><img src='images/lapras-GAN.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Single Image Super-resolution Reconstruction of Enhanced Loss Function with Multi-GPU Training](./files/Single_Image_Super-Resolution_Reconstruction_of_Enhanced_Loss_Function_with_Multi-GPU_Training.pdf)

Jianqiang Huang<sup>*, #</sup>, **Kai Li$^*$**, Xiaoying Wang.

<strong><span class='show_paper_citations' data='fHkHcMsAAAAJ:9yKSN-GCB0IC'></span></strong>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICDIP 2019</div><img src='images/esrgan-pro.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Single image super resolution based on generative adversarial networks](./files/111790T.pdf)

**Kai Li**, Liang Ye, Shenghao Yang, Jianqiang Huang<sup>#</sup>, Xiaoying Wang.

<strong><span class='show_paper_citations' data='fHkHcMsAAAAJ:Tyk-4Ss8FVUC'></span></strong>
</div>
</div>
