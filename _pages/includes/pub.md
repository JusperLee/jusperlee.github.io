
# 📝 Publications

> ( <sup>*</sup> equal contribution, <sup>#</sup> corresponding author)

# 2025

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR 2025</div><img src='./images/sonicsim.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[SonicSim: A customizable simulation platform for speech processing in moving sound source scenarios](https://openreview.net/pdf?id=Hx2ADQLi8M) **Kai Li**, Wendi Sang, Chang Zeng, Guo Che, Runxuan Yang, Xiaolin Hu. **ICLR 2025. Singapore EXPO.**

- SonicSim is a customizable simulation platform built on Habitat-sim, designed to generate high-fidelity, diverse synthetic data for speech separation and enhancement tasks involving moving sound sources, addressing the limitations of real-world and existing synthetic datasets in acoustic realism and scalability.

- <a href="https://cslikai.cn/SonicSim/"><img src="https://img.shields.io/badge/Demo_Page-Online-brightgreen" alt=""></a> \| <a href="https://github.com/JusperLee/SonicSim"><img src="https://img.shields.io/github/stars/JusperLee/SonicSim?style=social&amp;label=Code+Stars" alt=""></a> \| [![SonicSim Docker](https://img.shields.io/badge/Docker-SonicSim-blue?logo=docker)](https://hub.docker.com/r/jusperlee1/sonicsim/tags) \| [![SonicSet](https://img.shields.io/badge/Huggingface-SonicSet-orange?logo=huggingface)](https://huggingface.co/JusperLee/SonicSet) \| [![WeChat Article](https://img.shields.io/badge/Xinzhiyuan-新智元-green?logo=wechat)](https://mp.weixin.qq.com/s/7tZDu5nDBoG1h0YuXA1C2g)

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR 2025</div><img src='./images/tiger.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[TIGER: Time-frequency Interleaved Gain Extraction and Reconstruction for Efficient Speech Separation](https://openreview.net/pdf?id=rzx3vcvlzj) Mohan Xu, **Kai Li***, Guo Chen, Xiaolin Hu. **ICLR 2025. Singapore EXPO.**

- TIGER is an efficient time-frequency domain speech separation model that significantly reduces parameters and computational costs by leveraging frequency band-split and multi-scale attention, achieving state-of-the-art performance while being highly lightweight.

- <a href="https://cslikai.cn/TIGER/"><img src="https://img.shields.io/badge/Demo_Page-Online-brightgreen" alt=""></a> \| <a href="https://github.com/JusperLee/TIGER"><img src="https://img.shields.io/github/stars/JusperLee/TIGER?style=social&amp;label=Code+Stars" alt=""></a> \| [![TIGER Speech Model](https://img.shields.io/badge/Huggingface-TIGER_Speech-orange?logo=huggingface)](https://huggingface.co/JusperLee/TIGER-speech) \| [![TIGER DnR Model](https://img.shields.io/badge/Huggingface-TIGER_DnR-orange?logo=huggingface)](https://huggingface.co/JusperLee/TIGER-DnR)

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICASSP 2025</div><img src='./images/apollo.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Apollo: Band-sequence Modeling for High-Quality Audio Restoration](https://arxiv.org/pdf/2409.08514) **Kai Li**, Yi Luo. **ICASSP 2025. Hyderabad, India.**

- Apollo is a generative model for high-sample-rate audio restoration that utilizes a frequency band split module to enhance audio quality, outperforming existing SR-GAN models in terms of restoration quality and computational efficiency.

- <a href="https://cslikai.cn/Apollo/"><img src="https://img.shields.io/badge/Demo_Page-Online-brightgreen" alt=""></a> \| <a href="https://github.com/JusperLee/Apollo"><img src="https://img.shields.io/github/stars/JusperLee/Apollo?style=social&amp;label=Code+Stars" alt=""></a> \| [![Apollo Model](https://img.shields.io/badge/Huggingface-Apollo-orange?logo=huggingface)](https://huggingface.co/JusperLee/Apollo) \| [![Apollo Space](https://img.shields.io/badge/Huggingface-Apollo%20Space-blue?logo=huggingface)](https://huggingface.co/spaces/patriotyk/Apollo)

</div>
</div>

# 2024

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CCS 2024</div><img src='./images/safeear.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[SafeEar: Content Privacy-Preserving Audio Deepfake Detection](https://dl.acm.org/doi/abs/10.1145/3658644.3670285) Xinfeng Li, **Kai Li***, Yifan Zheng, Chen Yan, Xiaoyu Ji, Wenyuan Xu. **CCS 2024. Salt Lake City, U.S.A.**

- SafeEar, a novel framework that aims to detect deepfake audios without relying on accessing the speech content within.
  
- <a href="https://safeearweb.github.io/Project/"><img src="https://img.shields.io/badge/Demo_Page-Online-brightgreen" alt=""></a> \| <a href="https://github.com/LetterLiGo/SafeEar"><img src="https://img.shields.io/github/stars/LetterLiGo/SafeEar?style=social&amp;label=Code+Stars" alt=""></a> \| [![Zenodo](https://img.shields.io/badge/Zenodo-CVoiceFake_Full-blue?logo=zenodo)](https://zenodo.org/records/14062964) \| [![Zenodo](https://img.shields.io/badge/Zenodo-CVoiceFake-blue?logo=zenodo)](https://zenodo.org/records/11124319) \| [![WeChat Article](https://img.shields.io/badge/Xinzhiyuan-新智元-green?logo=wechat)](https://mp.weixin.qq.com/s/6OWv6nzYoiSRsSv79-FSbQ)


</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICML 2024</div><img src='./images/iianet.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[IIANet: an intra- and inter-modality attention network for audio-visual speech separation](https://openreview.net/pdf?id=FM61SQzF3N) **Kai Li**, Runxuan Yang, Sun Fuchun, Xiaolin Hu. **ICML 2024. Vienna, Austria.**

- Inspired by the cross-modal processing mechanism in the brain, we design intra- and inter-attention modules to integrate auditary and visual information for efficient speech separation. The model simulates audio-visual fusion in different levels of sensory cortical areas as well as higher association areas such as parietal cortex.

- <a href="https://cslikai.cn/IIANet/"><img src="https://img.shields.io/badge/Demo_Page-Online-brightgreen" alt=""></a> \| <a href="https://github.com/JusperLee/IIANet"><img src="https://img.shields.io/github/stars/JusperLee/IIANet?style=social&amp;label=Code+Stars" alt=""></a> \| [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/scanet-a-self-and-cross-attention-network-for/speech-separation-on-lrs2)](https://paperswithcode.com/sota/speech-separation-on-lrs2?p=scanet-a-self-and-cross-attention-network-for) \| [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/scanet-a-self-and-cross-attention-network-for/speech-separation-on-lrs3)](https://paperswithcode.com/sota/speech-separation-on-lrs3?p=scanet-a-self-and-cross-attention-network-for) \| [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/scanet-a-self-and-cross-attention-network-for/speech-separation-on-voxceleb2)](https://paperswithcode.com/sota/speech-separation-on-voxceleb2?p=scanet-a-self-and-cross-attention-network-for)

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Remote Sensing 2024</div><img src='https://www.mdpi.com/remotesensing/remotesensing-16-02899/article_deploy/html/images/remotesensing-16-02899-g004.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Towards Robust Pansharpening: A Large-Scale High-Resolution Multi-Scene Dataset and Novel Approach](https://www.mdpi.com/2072-4292/16/16/2899) Shiying Wang, Xuechao Zou, **Kai Li**, Junliang Xing, Tengfei Cao, Pin Tao. **Remote Sensing 2024.**

- PanBench, a high-resolution multi-scene dataset containing all mainstream satellites and comprising 5898 pairs of samples.

- <a href="https://github.com/XavierJiezou/Pansharpening"><img src="https://img.shields.io/github/stars/XavierJiezou/Pansharpening?style=social&amp;label=Code+Stars" alt=""></a> \| [![Huggingface-Dataset](https://img.shields.io/badge/Huggingface-Dataset-orange?logo=huggingface)](https://huggingface.co/datasets/XavierJiezou/pansharpening-datasets)

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ISMIR 2024</div><img src='./images/cinematic.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[The sound demixing challenge 2023–Cinematic demixing track.](https://arxiv.org/pdf/2308.06981) Stefan Uhlich, Giorgio Fabbro, Masato Hirano, Shusuke Takahashi, Gordon Wichern, Jonathan Le Roux, Dipam Chakraborty, Sharada Mohanty, **Kai Li**, Yi Luo, Jianwei Yu, Rongzhi Gu, Roman Solovyev, Alexander Stempkovskiy, Tatiana Habruseva, Mikhail Sukhovei, Yuki Mitsufuji. **ISMIR 2024.**

- This paper summarizes the cinematic demixing (CDX) track of the Sound Demixing Challenge 2023 (SDX’23). We provide a **comprehensive summary** of the challenge setup, detailing the structure of the competition and the datasets used.

- [![Sound Demixing Challenge](https://img.shields.io/badge/Sound%20Demixing-Challenge-blue?logo=audio&logoColor=white)](https://www.aicrowd.com/challenges/sound-demixing-challenge-2023/problems/cinematic-sound-demixing-track-cdx-23)

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Arxiv 2024</div><img src='./images/spmamba.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[SPMamba: State-space model is all you need in speech separation.](https://arxiv.org/pdf/2404.02063) Kai Li, Guo Chen. **Arxiv 2024.**

- SPMamba revolutionizes the field of speech separation tasks by leveraging the power of **Mamba** in conjunction with the robust **TF-GridNet** infrastructure.

- [![Demo-Page](https://img.shields.io/badge/Demo_Page-Online-brightgreen)](https://cslikai.cn/SPMamba/) \| <a href="https://github.com/JusperLee/SPMamba"><img src="https://img.shields.io/github/stars/JusperLee/SPMamba?style=social&amp;label=Code+Stars" alt=""></a> \| [![MITTR News](https://img.shields.io/badge/MITTR%20News-Article-blue?logo=read-the-docs)](https://www.mittrchina.com/news/detail/13425)

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICME 2024</div><img src='https://github.com/BastianChen/LEPrompter/raw/master/resources/leprompter_decoder.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[High-Fidelity Lake Extraction via Two-Stage Prompt Enhancement: Establishing a Novel Baseline and Benchmark.](https://arxiv.org/pdf/2308.08443) Ben Chen, Xuechao Zou, **Kai Li**, Yu Zhang, Junliang Xing, Pin Tao **ICME 2024. Niagra Falls, Canada**

- This paper presents LEPrompter, a **two-stage prompt-based** framework that enhances lake extraction from remote sensing imagery by using training prompts to improve segmentation accuracy and operating prompt-free during inference for efficient automated extraction.

- <a href="https://github.com/BastianChen/LEPrompter"><img src="https://img.shields.io/github/stars/BastianChen/LEPrompter?style=social&amp;label=Code+Stars" alt=""></a>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TPAMI 2024</div><img src='images/ctcnet.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[An Audio-Visual Speech Separation Model Inspired by Cortico-Thalamo-Cortical Circuits.](https://arxiv.org/abs/2212.10744) **Kai Li**, Fenghua Xie, Hang Chen, Kexin Yuan, Xiaolin Hu. **TPAMI 2024.**

- A **brain-inspired model** for audio-visual speech separation. The **state-of-the-art** model on this task.

-  [![Demo-Page](https://img.shields.io/badge/Demo_Page-Online-brightgreen)](./project/CTCNet) \| <a href="https://github.com/JusperLee/CTCNet"><img src="https://img.shields.io/github/stars/JusperLee/CTCNet?style=social&amp;label=Code+Stars" alt=""></a> \| [![Brain and Intelligence Laboratory](https://img.shields.io/badge/Brain%20and%20Intelligence%20Laboratory-脑与智能实验室-blue?logo=wechat)](https://mp.weixin.qq.com/s/Q51QOLswgVhdt2JBNI-hSQ)

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR 2024</div><img src='https://github.com/spkgyk/RTFS-Net/raw/master/docs/av-pipeline.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[RTFS-Net: Recurrent time-frequency modelling for efficient audio-visual speech separation.](https://arxiv.org/html/2309.17189v3) Samuel Pegg<sup>*</sup>, **Kai Li<sup>*</sup>**, Xiaolin Hu. **ICLR 2024. Vienna, Austria.**

- The core contribution of this paper is the development of the DiffCR framework, a novel **fast conditional diffusion model** for high-quality cloud removal from optical satellite images, which significantly outperforms existing models in both synthesis **quality and computational efficiency**.

- [![Demo-Page](https://img.shields.io/badge/Demo_Page-Online-brightgreen)](https://anonymous.4open.science/w/RTFS-Net/AV-Model-Demo.html) \| <a href="https://github.com/spkgyk/RTFS-Net"><img src="https://img.shields.io/github/stars/spkgyk/RTFS-Net?style=social&amp;label=Code+Stars" alt=""></a> \| [![Jiqizhixin Article](https://img.shields.io/badge/Jiqizhixin-Article-blue?logo=read-the-docs)](https://www.jiqizhixin.com/articles/2024-03-06)


</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TGRS 2024</div><img src='https://github.com/XavierJiezou/DiffCR/raw/main/image/README/diffcr.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[DiffCR: A Fast Conditional Diffusion Framework for Cloud Removal From Optical Satellite Images.](https://arxiv.org/pdf/2308.04417) Xuechao Zou<sup>*</sup>, **Kai Li<sup>*</sup>**, Junliang Xing, Yu Zhang, Shiying Wang, Lei Jin, Pin Tao. **TGRS 2024.**

- The core contribution of this paper is the development of the DiffCR framework, a novel **fast conditional diffusion model** for high-quality cloud removal from optical satellite images, which significantly outperforms existing models in both synthesis **quality and computational efficiency**.

- [![Demo-Page](https://img.shields.io/badge/Demo_Page-Online-brightgreen)](https://xavierjiezou.github.io/DiffCR/) \| <a href="https://github.com/XavierJiezou/DiffCR"><img src="https://img.shields.io/github/stars/XavierJiezou/DiffCR?style=social&amp;label=Code+Stars" alt=""></a>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICASSP 2024</div><img src='images/sub-to-go.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Subnetwork-to-go: Elastic Neural Network with Dynamic Training and Customizable Inference.](https://arxiv.org/pdf/2312.03464) **Kai Li**, Yi Luo. **ICASSP 2024. Seoul, Korea.**

- A method for training neural networks with **dynamic depth and width** configurations, enabling flexible extraction of subnetworks during inference **without additional training**.

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICASSP 2024</div><img src='images/leformer.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[LEFormer: A Hybrid CNN-Transformer Architecture for Accurate Lake Extraction from Remote Sensing Imagery.](https://arxiv.org/pdf/2308.04397) Ben Chen, Xuechao Zou, Yu Zhang, Jiayu Li, **Kai Li**, Pin Tao. **ICASSP 2024. Seoul, Korea.**

- The core contribution of this paper is the introduction of LEFormer, a **hybrid CNN-Transformer** architecture that effectively combines local and global features for high-precision lake extraction from remote sensing images, achieving **state-of-the-art performance and efficiency** on benchmark datasets.

- <a href="https://github.com/BastianChen/LEFormer"><img src="https://img.shields.io/github/stars/BastianChen/LEFormer?style=social&amp;label=Code+Stars" alt=""></a>

</div>
</div>

# 2023

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICIST 2023</div><img src='images/tdfnet.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[TDFNet: An Efficient Audio-Visual Speech Separation Model with Top-down Fusion.](https://arxiv.org/pdf/2401.14185) Samuel Pegg<sup>*</sup>, **Kai Li<sup>*</sup>**, Xiaolin Hu. **ICIST 2023. Cairo, Egypt.**

- TDFNet is a cutting-edge method in the field of audio-visual speech separation. It introduces a **multi-scale and multi-stage** framework, leveraging the strengths of TDANet and CTCNet. This model is designed to address the inefficiencies and limitations of existing multimodal speech separation models, particularly in real-time tasks.

- <a href="https://github.com/spkgyk/TDFNet"><img src="https://img.shields.io/github/stars/spkgyk/TDFNet?style=social&amp;label=Code+Stars" alt=""></a>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ECAI 2023</div><img src='images/pmaa.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[PMAA: A Progressive Multi-scale Attention Autoencoder Model for High-Performance Cloud Removal from Multi-temporal Satellite Imagery.](https://arxiv.org/pdf/2303.16565.pdf) Xuechao Zou<sup>*</sup>, **Kai Li<sup>*</sup>**, Junliang Xing, Pin Tao<sup>#</sup>, Yachao Cui. **ECAI 2023. Kraków, Poland.**

- A **Progressive Multi-scale Attention Autoencoder (PMAA)** model for high-performance cloud removal from multi-temporal satellite imagery, which achieves **state-of-the-art performance** on benchmark datasets.
- [![Demo-Page](https://img.shields.io/badge/Demo_Page-Online-brightgreen)](https://xavierjiezou.github.io/PMAA/) \| <a href="https://github.com/XavierJiezou/PMAA"><img src="https://img.shields.io/github/stars/XavierJiezou/PMAA?style=social&amp;label=Code+Stars" alt=""></a>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR 2023</div><img src='images/tdanet.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[An efficient encoder-decoder architecture with top-down attention for speech separation.](https://arxiv.org/pdf/2209.15200) **Kai Li**, Runxuan Yang, Xiaolin Hu. **ICLR 2023. Kigali, Rwanda.**

- **Top-down neural projections** are ubiquitous in the brain. We found that this kind of projections are very useful for solving the Cocktail Party Problem.

- [![知乎](https://img.shields.io/badge/知乎-TDANet(ICLR 2023)-0084FF.svg)](https://zhuanlan.zhihu.com/p/605100121) \| 
[![Demo-Page](https://img.shields.io/badge/Demo_Page-Online-brightgreen)](./project/TDANet) \| <a href="https://github.com/JusperLee/TDANet"><img src="https://img.shields.io/github/stars/JusperLee/TDANet?style=social&amp;label=Code+Stars" alt=""></a> <strong><span class='show_paper_citations' data='fHkHcMsAAAAJ:LkGwnXOMwfcC'></span></strong>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">SDX Workshop 2023</div><img src='images/sdx.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[One-page Report for Tencent AI Lab’s CDX 2023 System.](https://sdx-workshop.github.io/papers/Li.pdf) **Kai Li**, Yi Luo. **SDX Workshop 2023. Paris, France.**

- We present the system of Tencent AI Lab for the Cinematic Sound Demixing Challenge 2023, which is based on a **novel neural network architecture and a new training strategy**.

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Interspeech 2023</div><img src='images/avlit.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Audio-Visual Speech Separation in Noisy Environments with a Lightweight Iterative Model.](https://arxiv.org/pdf/2306.00160) Héctor Martel, Julius Richter, **Kai Li**, Xiaolin Hu and Timo Gerkmann. **Interspeech 2023.  Dublin, Ireland.**

- An audio-visual **lightweight iterative model** for speech separation in noisy environments. AVLIT employs **Progressive Learning (PL)** to decompose the mapping between inputs and outputs into multiple steps, enhancing computational efficiency. 

- [![Demo-Page](https://img.shields.io/badge/Demo_Page-Online-brightgreen)](https://avlit-interspeech.github.io/) \| <a href="https://github.com/hmartelb/avlit"><img src="https://img.shields.io/github/stars/hmartelb/avlit?style=social&amp;label=Code+Stars" alt=""></a>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Interspeech 2023</div><img src='images/s4m.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[A Neural State-Space Model Approach to Efficient Speech Separation.](https://arxiv.org/pdf/2305.16932) Chenchen, Chao-Han Huck Yang, **Kai Li**, Yuchen Hu, Pin-Jui Ku and Eng Siong Chng. **Interspeech 2023.  Dublin, Ireland.**

- We propose a **Neural State-Space Model (S4M)** for speech separation, which can **efficiently** separate multiple speakers.

- <a href="https://github.com/JusperLee/S4M"><img src="https://img.shields.io/github/stars/JusperLee/S4M?style=social&amp;label=Code+Stars" alt=""></a>

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICASSP 2023</div><img src='images/causal.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[On the Design and Training Strategies for RNN-based Online Neural Speech Separation Systems.](https://arxiv.org/pdf/2206.07340) **Kai Li**, Yi Luo. **ICASSP 2023. Melbourne, Australia.**

- The paper explores converting **RNN-based** offline neural speech separation systems to **online systems** with minimal performance degradation.

</div>
</div>

# 2022

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">InterSpeech 2022</div><img src='images/overss.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
  
[On the Use of Deep Mask Estimation Module for Neural Source Separation Systems.](http://arxiv.org/pdf/2206.07347v1) **Kai Li**, Xiaolin Hu, Yi Luo. **InterSpeech 2022. Incheon, Korea.**

- We propose a **Deep Mask Estimation Module** for speech separation, which can **improve the performance** without additional computional complex.

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Neural Computation 2022</div><img src='images/nerualcom.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Inferring mechanisms of auditory attentional modulation with deep neural networks](./files/neural.pdf) Ting-Yu Kuo, Yuanda Liao, **Kai Li**, Bo Hong, Xiaolin Hu. **Neural Computation, 2022.**

- With the help of DNNs, we suggest that the projection of **top-down attention** signals to lower stages within the auditory pathway of the human brain plays a more significant role than the higher stages in solving the "cocktail party problem".

- <a href="https://github.com/liaoyd16/cocktail_lk"><img src="https://img.shields.io/github/stars/liaoyd16/cocktail_lk?style=social&amp;label=Code+Stars" alt=""></a><strong>

</div>
</div>

# 2021

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2021</div><img src='images/afrcnn.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Speech Separation Using an Asynchronous Fully Recurrent Convolutional Neural Network.](https://papers.nips.cc/paper/2021/file/be1bc7997695495f756312886f566110-Paper.pdf) Xiaolin Hu<sup>*, #</sup>, **Kai Li$^*$**, Weiyi Zhang, Yi Luo, Jean-Marie Lemercier, Timo Gerkmann. **NeurIPS 2021. Online.**

- This paper introduces an asynchronous updating scheme in a **bio-inspired recurrent neural network architecture**, significantly enhancing speech separation **efficiency and accuracy** compared to traditional synchronous methods.

- [![知乎](https://img.shields.io/badge/知乎-AFRCNN(NeuralPS 2021)-0084FF.svg)](https://zhuanlan.zhihu.com/p/508868699) \| [![Demo-Page](https://img.shields.io/badge/Sep_Demo_Page-Online-brightgreen)](./project/AFRCNN) \| [![Enh-Demo-Page](https://img.shields.io/badge/Enh_Demo_Page-Online-brightgreen)](./project/AFRCNN-Enh) \| <a href="https://github.com/JusperLee/AFRCNN-For-Speech-Separation"><img src="https://img.shields.io/github/stars/JusperLee/AFRCNN-For-Speech-Separation?style=social&amp;label=Code+Stars" alt=""></a> <strong><span class='show_paper_citations' data='fHkHcMsAAAAJ:zYLM7Y9cAGgC'></span></strong>

</div>
</div>

# 2020 and Prior


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IET Image Processing 2020</div><img src='images/resolution_servey.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[A Survey of Single Image Super Resolution Reconstruction.](./files/A_Survey_of_Single_Image_Super_Resolution_Reconstr.pdf) **Kai Li**, Shenghao Yang, Runting Dong, Jianqiang Huang<sup>#</sup>, Xiaoying Wang. **IET Image Processing 2020.**

- This paper provides a comprehensive survey of **single image super-resolution reconstruction** methods, including **traditional methods** and **deep learning-based methods**.

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ISPA 2019</div><img src='images/lapras-GAN.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Single Image Super-resolution Reconstruction of Enhanced Loss Function with Multi-GPU Training.](./files/Single_Image_Super-Resolution_Reconstruction_of_Enhanced_Loss_Function_with_Multi-GPU_Training.pdf) Jianqiang Huang<sup>*, #</sup>, **Kai Li$^*$**, Xiaoying Wang.

- This paper proposes a **multi-GPU training** method for single image super-resolution reconstruction, which can significantly **reduce training time** and **improve performance**.

</div>
</div>
